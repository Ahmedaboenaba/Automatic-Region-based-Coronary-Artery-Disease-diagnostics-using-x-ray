{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"authorship_tag":"ABX9TyPF74xxmlQyDz4R2B+kEJ4E","gpuType":"V28","name":"","version":""},"accelerator":"TPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":10200208,"sourceType":"datasetVersion","datasetId":6303094}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ahmedaboenaba/vgg-colorectal-cancer-classification?scriptVersionId=215787395\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Multi-tissue classification colorectal cancer","metadata":{"id":"idblyE8Aay0B"}},{"cell_type":"markdown","source":"**Table of Contents**                                                                      \nStep1: Environment Setup                                                                          \nStep2: Data Loading and Preparation                                                                      \nStep3: Analyzing Class Distribution                                                          \nStep4: Data Augmentation for Minority Classes                                                            \nStep5: Splitting the Dataset                                                                                     \nStep6: Preparing Data Generators                                                             \nStep7: Building the Model                                                                                       \nStep8: Compiling the Model with Class Weights                                                  \nStep9: Implementing Learning Rate Scheduling and Early Stopping                                      \nStep10: Training the Model                                                             \nStep11: Evaluating the Model                                                                   \nStep12: Conclusion and Next Steps                                                           \n\n","metadata":{"id":"sooOx_XlbLh7"}},{"cell_type":"markdown","source":"**Step: 1 Environment Setup**","metadata":{"id":"Sz3eUdQ2beDY"}},{"cell_type":"code","source":"# Data manipulation and analysis\nimport os \nimport numpy as np\nimport pandas as pd\nfrom glob import glob\n\n# Image processing\nfrom PIL import Image\n\n# Machine learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# Deep learning\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n#from tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization, MaxPool2D, Input\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\n\n\n# Callbacks\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\n# Visualization\nimport matplotlib.pyplot as plt","metadata":{"executionInfo":{"elapsed":3593,"status":"ok","timestamp":1735675897038,"user":{"displayName":"Ahmed Jaber Aboenaba","userId":"17055837399030752845"},"user_tz":-120},"id":"nLYUnQi2Njl_","trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:49:47.803449Z","iopub.execute_input":"2025-01-02T11:49:47.803781Z","iopub.status.idle":"2025-01-02T11:49:47.809259Z","shell.execute_reply.started":"2025-01-02T11:49:47.803751Z","shell.execute_reply":"2025-01-02T11:49:47.808335Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 2: Loading Data**\n","metadata":{"id":"aLrXoKCudv-T"}},{"cell_type":"code","source":"# Assuming the dataset is in a folder named 'dataset'\ndata_dir = '/kaggle/input/No_Norm'\n\n# Get class names from subdirectory names\nclasses = sorted([d.name for d in os.scandir(data_dir) if d.is_dir()])\nprint(\"Classes:\", classes)\n\n\n# Initialize lists to hold file paths and labels\nimage_paths = []\nimage_labels = []\n\n# Map class names to numerical labels\nclass_to_label = {class_name: idx for idx, class_name in enumerate(classes)}\nprint(\"Class to label mapping:\", class_to_label)\n\n# Loop through each class directory and collect file paths\nfor class_name in classes:\n    class_path = os.path.join(data_dir, class_name)\n    # Get all image file paths in the class directory\n    image_files = [os.path.join(class_path, f) for f in os.listdir(class_path) if f.lower().endswith('.png')]\n    # Append file paths and labels\n    image_paths.extend(image_files)\n    image_labels.extend([class_to_label[class_name]] * len(image_files))\n    #print(f\"Processing {len(image_files)} images for class '{class_name}'\")\n\nprint(f\"Total images found: {len(image_paths)}\")","metadata":{"id":"h12JQ5r3lB-p","trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:47:38.494852Z","iopub.execute_input":"2025-01-02T11:47:38.495117Z","iopub.status.idle":"2025-01-02T11:47:38.526298Z","shell.execute_reply.started":"2025-01-02T11:47:38.495098Z","shell.execute_reply":"2025-01-02T11:47:38.525489Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3. Analyzing Class Distribution**","metadata":{}},{"cell_type":"code","source":"# Create a DataFrame to aid in analysis\ndf = pd.DataFrame({\n    'image_path': image_paths,\n    'label': image_labels\n})\n\n# Map numerical labels to class names\nlabel_to_class = {v: k for k, v in class_to_label.items()}\ndf['class_name'] = df['label'].map(label_to_class)\n\n# Display class counts\nclass_counts = df['class_name'].value_counts()\nprint(\"Class counts before augmentation:\\n\", class_counts)\n\n# Plot class distribution\nclass_counts.plot(kind='barh')\nplt.title('Class Distribution Before Augmentation')\nplt.xlabel('Number of Images')\nplt.ylabel('Class Name')\nplt.xticks(rotation=45)\nplt.show() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:47:38.527323Z","iopub.execute_input":"2025-01-02T11:47:38.527577Z","iopub.status.idle":"2025-01-02T11:47:38.759253Z","shell.execute_reply.started":"2025-01-02T11:47:38.527559Z","shell.execute_reply":"2025-01-02T11:47:38.758501Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**4. Data Augmentation for Minority Classes**\n","metadata":{"id":"Q1EeD9lt1YZD"}},{"cell_type":"code","source":"# Define augmentation parameters\ndata_gen_args = dict(\n    preprocessing_function=preprocess_input,  # Required for VGG\n    rotation_range=20,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=True,\n    vertical_flip=True,\n    zoom_range=0.1,\n    fill_mode='nearest'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:47:46.892491Z","iopub.execute_input":"2025-01-02T11:47:46.892774Z","iopub.status.idle":"2025-01-02T11:47:46.896734Z","shell.execute_reply.started":"2025-01-02T11:47:46.892754Z","shell.execute_reply":"2025-01-02T11:47:46.895884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define data augmentation transformations\ndef balanced_data_generator(df, batch_size, target_size, datagen):\n    '''\n    A generator that yields batches of images and labels, balancing classes by oversampling minority classes.\n    '''\n    # Get the class counts\n    class_counts = df['label'].value_counts()\n    max_count = class_counts.max()\n    class_indices = {label: df[df['label'] == label].index for label in class_counts.index}\n\n    while True:\n        batch_paths = []\n        batch_labels = []\n\n        # Oversample minority classes\n        for label, indices in class_indices.items():\n            num_samples = max_count - len(indices)\n            if num_samples > 0:\n                extra_indices = np.random.choice(indices, size=num_samples, replace=True)\n                indices = indices.union(extra_indices)\n\n            # Shuffle indices\n            indices = np.random.permutation(indices)\n            batch_paths.extend(df.loc[indices, 'image_path'].tolist())\n            batch_labels.extend(df.loc[indices, 'label'].tolist())\n\n        # Combine and shuffle\n        combined = list(zip(batch_paths, batch_labels))\n        np.random.shuffle(combined)\n        batch_paths, batch_labels = zip(*combined)\n\n        # Yield batches\n        for i in range(0, len(batch_paths), batch_size):\n            batch_end = min(i + batch_size, len(batch_paths))\n            batch_images = []\n            batch_labels_one_hot = []\n\n            for j in range(i, batch_end):\n                # Load and preprocess the image\n                img = Image.open(batch_paths[j]).convert('RGB')\n                img = img.resize(target_size)\n                img_array = np.array(img)\n                batch_images.append(img_array)\n                batch_labels_one_hot.append(to_categorical(batch_labels[j], num_classes=len(classes)))\n\n            # Convert lists to arrays\n            X_batch = np.array(batch_images)\n            y_batch = np.array(batch_labels_one_hot)\n\n            # Apply real-time data augmentation\n            augmented_iterator = datagen.flow(X_batch, y_batch, batch_size=batch_size, shuffle=False)\n            X_batch_augmented, y_batch_augmented = next(augmented_iterator)\n\n            # Normalize\n            X_batch_augmented = X_batch_augmented / 255.0\n\n            yield X_batch_augmented, y_batch_augmented","metadata":{"id":"CotVoJk-1XX0","trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:47:48.118864Z","iopub.execute_input":"2025-01-02T11:47:48.119218Z","iopub.status.idle":"2025-01-02T11:47:48.12704Z","shell.execute_reply.started":"2025-01-02T11:47:48.11919Z","shell.execute_reply":"2025-01-02T11:47:48.126324Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 5: Split the Dataset**\n","metadata":{"id":"bZF_t9Dpx24q"}},{"cell_type":"code","source":"# Split into training and temp (validation + test)\ntrain_df, temp_df = train_test_split(\n    df,\n    test_size=0.2,\n    stratify=df['label'],\n    random_state=42\n)\n\n# Split temp into validation and test\nval_df, test_df = train_test_split(\n    temp_df,\n    test_size=0.5,\n    stratify=temp_df['label'],\n    random_state=42\n)\n\nprint(f\"Training set size: {len(train_df)}\")\nprint(f\"Validation set size: {len(val_df)}\")\nprint(f\"Test set size: {len(test_df)}\")\n\n\n","metadata":{"id":"dsK_WCvox34-","trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:47:52.457747Z","iopub.execute_input":"2025-01-02T11:47:52.458071Z","iopub.status.idle":"2025-01-02T11:47:52.469654Z","shell.execute_reply.started":"2025-01-02T11:47:52.458009Z","shell.execute_reply":"2025-01-02T11:47:52.468837Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**6. Preparing Data Generators**\n","metadata":{}},{"cell_type":"code","source":"# Parameters\nbatch_size = 32\ntarget_size = (150, 150)  # VGG input size\nnum_classes = len(classes)\n\n# Data generators\n# Create an instance of ImageDataGenerator with the defined transformations\ndatagen = ImageDataGenerator(**data_gen_args)\nval_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\ntest_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:47:58.378355Z","iopub.execute_input":"2025-01-02T11:47:58.378639Z","iopub.status.idle":"2025-01-02T11:47:58.383078Z","shell.execute_reply.started":"2025-01-02T11:47:58.378618Z","shell.execute_reply":"2025-01-02T11:47:58.382119Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"6.1. Training Data Generator\n","metadata":{}},{"cell_type":"code","source":"# Training generator with balanced batches\ntrain_generator = balanced_data_generator(\n    train_df,\n    batch_size=batch_size,\n    target_size=target_size,\n    datagen=datagen\n)\n\n\n# Calculate steps per epoch\n#steps_per_epoch = len(train_df) * 2 // batch_size  # Oversampling will effectively double the data\n\nsteps_per_epoch = (train_df['label'].value_counts().max() * num_classes) // batch_size\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:48:00.963403Z","iopub.execute_input":"2025-01-02T11:48:00.963718Z","iopub.status.idle":"2025-01-02T11:48:00.968591Z","shell.execute_reply.started":"2025-01-02T11:48:00.963692Z","shell.execute_reply":"2025-01-02T11:48:00.967829Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"6.2. Validation Data Generator","metadata":{}},{"cell_type":"code","source":"# Validation data preparation\ndef prepare_dataset(df):\n    images = []\n    labels = []\n\n    for idx, row in df.iterrows():\n        img = Image.open(row['image_path']).convert('RGB')\n        img = img.resize(target_size)\n        img_array = np.array(img)\n        images.append(img_array)\n        labels.append(row['label'])\n\n    X = np.array(images) / 255.0\n    y = to_categorical(labels, num_classes=len(classes))\n    return X, y\n\n# Prepare validation and test datasets\nX_val, y_val = prepare_dataset(val_df)\nX_test, y_test = prepare_dataset(test_df)\n\nprint(f\"Validation set shape: {X_val.shape}, {y_val.shape}\")\nprint(f\"Test set shape: {X_test.shape}, {y_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:48:04.037537Z","iopub.execute_input":"2025-01-02T11:48:04.037822Z","iopub.status.idle":"2025-01-02T11:48:07.202433Z","shell.execute_reply.started":"2025-01-02T11:48:04.037801Z","shell.execute_reply":"2025-01-02T11:48:07.201571Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**9. Building and Configuring the VGG Model**","metadata":{"id":"Dx0nHAAIlod4"}},{"cell_type":"code","source":"visible = Input(shape=(150, 150, 3))\n#######\nlayer_in = Conv2D(64, 1, padding='same', activation='relu')(visible)\nlayer_in = Conv2D(64, 1, padding='same', activation='relu')(layer_in)\nlayer_in = MaxPooling2D((2,2), strides=(2,2))(layer_in)\n#######\nlayer_in = Conv2D(128, 1, padding='same', activation='relu')(layer_in)\nlayer_in = Conv2D(128, 1, padding='same', activation='relu')(layer_in)\nlayer_in = MaxPooling2D((2,2), strides=(2,2))(layer_in)\n#######\nlayer_in = Conv2D(256, 1, padding='same', activation='relu')(layer_in)\nlayer_in = Conv2D(256, 1, padding='same', activation='relu')(layer_in)\nlayer_in = Conv2D(256, 1, padding='same', activation='relu')(layer_in)\nlayer_in = Conv2D(256, 1, padding='same', activation='relu')(layer_in)\nlayer_in = MaxPooling2D((2,2), strides=(2,2))(layer_in)\n#######\nlayer_in = Conv2D(512, 1, padding='same', activation='relu')(layer_in)\nlayer_in = Conv2D(512, 1, padding='same', activation='relu')(layer_in)\nlayer_in = Conv2D(512, 1, padding='same', activation='relu')(layer_in)\nlayer_in = Conv2D(512, 1, padding='same', activation='relu')(layer_in)\nlayer_in = MaxPooling2D((2,2), strides=(2,2))(layer_in)\n#######\nlayer_in = Conv2D(512, 1, padding='same', activation='relu')(layer_in)\nlayer_in = Conv2D(512, 1, padding='same', activation='relu')(layer_in)\nlayer_in = Conv2D(512, 1, padding='same', activation='relu')(layer_in)\nlayer_in = Conv2D(512, 1, padding='same', activation='relu')(layer_in)\n#######\nlayer_in = MaxPooling2D((2,2), strides=(2,2))(layer_in)\nlayer_in = Flatten()(layer_in)\nlayer_in = Dense(4096, activation='relu' )(layer_in)\nlayer_in = Dropout(0.5)(layer_in)\nlayer_in = Dense(4096, activation='relu' )(layer_in)\nlayer_in = Dropout(0.5)(layer_in)\nlayer_in = Dense(6, activation='softmax' )(layer_in)\nmodel = Model(inputs=visible, outputs=layer_in)\n\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:49:54.987557Z","iopub.execute_input":"2025-01-02T11:49:54.987834Z","iopub.status.idle":"2025-01-02T11:49:56.295327Z","shell.execute_reply.started":"2025-01-02T11:49:54.987814Z","shell.execute_reply":"2025-01-02T11:49:56.294623Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compile the model\nmodel.compile(optimizer= 'adam' , loss= tf.keras.losses.binary_crossentropy, metrics=['accuracy']) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:50:02.945624Z","iopub.execute_input":"2025-01-02T11:50:02.945906Z","iopub.status.idle":"2025-01-02T11:50:02.959038Z","shell.execute_reply.started":"2025-01-02T11:50:02.945885Z","shell.execute_reply":"2025-01-02T11:50:02.958316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Early stopping to prevent overfitting\nearly_stopping = EarlyStopping(\n    monitor='val_loss',\n    patience=15,\n    restore_best_weights=True\n)\n\n# Reduce learning rate when a metric has stopped improving\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=10,\n    min_lr=1e-6,\n    #verbose=0\n)\n\ncallbacks = [early_stopping, reduce_lr]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:50:09.957303Z","iopub.execute_input":"2025-01-02T11:50:09.957584Z","iopub.status.idle":"2025-01-02T11:50:09.961791Z","shell.execute_reply.started":"2025-01-02T11:50:09.957562Z","shell.execute_reply":"2025-01-02T11:50:09.960992Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Step 7: Cross-validating model        \n","metadata":{"id":"RCdbHfQOlvZI"}},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=steps_per_epoch,\n    epochs=30,\n    validation_data=(X_val, y_val),\n    #class_weight=class_weights,\n    callbacks=callbacks\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-02T12:09:57.332Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Step 8: Testing model            \n","metadata":{"id":"28poMN7ilvzq"}},{"cell_type":"code","source":"# Evaluate the model on the test set\ntest_steps = len(test_df) // batch_size\ntest_loss, test_accuracy = model.evaluate(test_generator, steps=test_steps)\nprint(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")","metadata":{"id":"FfUi6rln1F8a","trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:47:46.52582Z","iopub.status.idle":"2025-01-02T11:47:46.526109Z","shell.execute_reply":"2025-01-02T11:47:46.525952Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\n\n# Generate classification report and confusion matrix\ny_pred = model.predict(X_test)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true_classes = np.argmax(y_test, axis=1)\n\nprint(\"Classification Report:\")\nprint(classification_report(y_true_classes, y_pred_classes))\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_true_classes, y_pred_classes))\n\n# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","metadata":{"id":"tI61z0vZ1Nx_","trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:47:46.526962Z","iopub.status.idle":"2025-01-02T11:47:46.527316Z","shell.execute_reply":"2025-01-02T11:47:46.527207Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}